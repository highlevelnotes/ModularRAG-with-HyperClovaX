{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 250428 테스트\n",
        "* HyperClovaX 테스트(임베딩 및 Chat Model 확인)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6GUHybGwnsPh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import jsonlines\n",
        "from langchain.schema import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_upstage import UpstageEmbeddings\n",
        "from langchain_milvus.vectorstores import Milvus\n",
        "from uuid import uuid4\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "import pandas as pd\n",
        "import pytz\n",
        "\n",
        "from datasets import Dataset\n",
        "from datetime import timedelta\n",
        "from operator import itemgetter\n",
        "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain.chains.query_constructor.base import (\n",
        "  StructuredQueryOutputParser,\n",
        "  get_query_constructor_prompt\n",
        ")\n",
        "from langchain_teddynote.evaluator import GroundednessChecker\n",
        "from langchain.retrievers.self_query.milvus import MilvusTranslator\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "import warnings\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello there! How can I assist you today?', additional_kwargs={}, response_metadata={'stop_reason': 'stop_before', 'input_length': 4, 'output_length': 12, 'seed': 2749831690, 'ai_filter': None}, id='run-33bb7d92-a56a-4e34-bf12-745801ad3522-0', usage_metadata={'input_tokens': 4, 'output_tokens': 12, 'total_tokens': 16})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.chat_models import ChatClovaX\n",
        "\n",
        "chat = ChatClovaX(\n",
        "  model='HCX-003'\n",
        ")\n",
        "\n",
        "chat.invoke('hi!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import ClovaXEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LP2hMGEp0B2P"
      },
      "outputs": [],
      "source": [
        "embeddings = ClovaXEmbeddings(\n",
        "    model='bge-m3'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "SPIguiAtnsPp"
      },
      "outputs": [],
      "source": [
        "def save_docs_to_jsonl(documents, file_path):\n",
        "    with jsonlines.open(file_path, mode=\"w\") as writer:\n",
        "        for doc in documents:\n",
        "            writer.write(doc.dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "9cCVM_fUcsV4"
      },
      "outputs": [],
      "source": [
        "def adjust_time_filter_to_week(time_filter):\n",
        "    \"\"\"\n",
        "    특정 날짜(YYYY-MM-DD)가 주어진 경우, 해당 날짜를 포함하는 주(월~일)의\n",
        "    첫 번째 날(월요일)과 마지막 날(일요일)로 변환하는 함수.\n",
        "\n",
        "    :param time_filter: dict, {\"start_date\": datetime, \"end_date\": datetime}\n",
        "    :return: dict, {\"start_date\": datetime, \"end_date\": datetime}\n",
        "    \"\"\"\n",
        "    # Extract start_date and end_date from time_filter\n",
        "    start_date = time_filter.start_date\n",
        "    end_date = time_filter.end_date\n",
        "\n",
        "    # Handle the case where start_date or end_date is None\n",
        "    if start_date is None or end_date is None:\n",
        "        if start_date is not None and end_date is None:\n",
        "            start_of_week = start_date - timedelta(days=start_date.weekday())  # 월요일 찾기\n",
        "            end_of_week = start_of_week + timedelta(days=6)  # 해당 주 일요일 찾기\n",
        "\n",
        "            return {\n",
        "                \"start_date\": start_of_week.replace(hour=0, minute=0, second=0),\n",
        "                \"end_date\": end_of_week.replace(hour=23, minute=59, second=59)\n",
        "            }\n",
        "        elif end_date is not None and start_date is None:\n",
        "            start_of_week = end_date - timedelta(days=end_date.weekday())  # 월요일 찾기\n",
        "            end_of_week = start_of_week + timedelta(days=6)  # 해당 주 일요일 찾기\n",
        "\n",
        "            return {\n",
        "                \"start_date\": start_of_week.replace(hour=0, minute=0, second=0),\n",
        "                \"end_date\": end_of_week.replace(hour=23, minute=59, second=59)\n",
        "            }\n",
        "        else:\n",
        "            return None  # or return the time_filter as is if you prefer\n",
        "\n",
        "    # 날짜가 동일한 경우, 주의 첫 번째 날(월요일)과 마지막 날(일요일)로 변경\n",
        "    if start_date.year == end_date.year and start_date.month==end_date.month and start_date.day==end_date.day:\n",
        "        start_of_week = start_date - timedelta(days=start_date.weekday())  # 월요일 찾기\n",
        "        end_of_week = start_of_week + timedelta(days=6)  # 해당 주 일요일 찾기\n",
        "\n",
        "        return {\n",
        "            \"start_date\": start_of_week.replace(hour=0, minute=0, second=0),\n",
        "            \"end_date\": end_of_week.replace(hour=23, minute=59, second=59)\n",
        "        }\n",
        "\n",
        "    # 날짜가 다르면 기존 time_filter 유지\n",
        "    return {\n",
        "        \"start_date\": start_date,\n",
        "        \"end_date\": end_date\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "oX62Lo_GcsV6"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "class TimeFilter(BaseModel):\n",
        "    start_date: Optional[datetime] = None\n",
        "    end_date: Optional[datetime] = None\n",
        "\n",
        "class SearchQuery(BaseModel):\n",
        "    query: str\n",
        "    time_filter: TimeFilter\n",
        "\n",
        "class Label(BaseModel):\n",
        "    chunk_id: int = Field(description=\"The unique identifier of the text chunk\")\n",
        "    chain_of_thought: str = Field(\n",
        "        description=\"The reasoning process used to evaluate the relevance\"\n",
        "    )\n",
        "    relevancy: int = Field(\n",
        "        description=\"Relevancy score from 0 to 10, where 10 is most relevant\",\n",
        "        ge=0,\n",
        "        le=10,\n",
        "    )\n",
        "\n",
        "class RerankedResults(BaseModel):\n",
        "    labels: list[Label] = Field(description=\"List of labeled and ranked chunks\")\n",
        "\n",
        "    @field_validator(\"labels\")\n",
        "    @classmethod\n",
        "    def model_validate(cls, v: list[Label]) -> list[Label]:\n",
        "        return sorted(v, key=lambda x: x.relevancy, reverse=True)\n",
        "\n",
        "def rerank_results(query: str, chunks: list[dict]) -> RerankedResults:\n",
        "    client = instructor.from_openai(OpenAI())\n",
        "    return client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        response_model=RerankedResults,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"\n",
        "                You are an expert search result ranker. Your task is to evaluate the relevance of each text chunk to the given query and assign a relevancy score.\n",
        "\n",
        "                For each chunk:\n",
        "                1. Analyze its content in relation to the query.\n",
        "                2. Provide a chain of thought explaining your reasoning.\n",
        "                3. Assign a relevancy score from 0 to 10, where 10 is most relevant.\n",
        "\n",
        "                Be objective and consistent in your evaluations.\n",
        "                \"\"\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\"\"\n",
        "                <query>{{ query }}</query>\n",
        "\n",
        "                <chunks_to_rank>\n",
        "                {% for chunk in chunks %}\n",
        "                <chunk id=\"{{ chunk.id }}\">\n",
        "                    {{ chunk.text }}\n",
        "                </chunk>\n",
        "                {% endfor %}\n",
        "                </chunks_to_rank>\n",
        "\n",
        "                Please provide a RerankedResults object with a Label for each chunk.\n",
        "                \"\"\",\n",
        "            },\n",
        "        ],\n",
        "        context={\"query\": query, \"chunks\": chunks},\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "7kaaAvpXcsV7"
      },
      "outputs": [],
      "source": [
        "def get_query_date(question):\n",
        "    today = datetime(2025, 1, 25)\n",
        "    days_since_last_friday = (today.weekday() - 4) % 7\n",
        "    last_friday = today - timedelta(days=days_since_last_friday)\n",
        "    issue_date = last_friday.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    client = instructor.from_openai(OpenAI())\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"o1\",\n",
        "        response_model=SearchQuery,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"\n",
        "                You are an AI assistant that extracts date ranges from financial queries.\n",
        "                The current report date is {issue_date}.\n",
        "                Your task is to extract the relevant date or date range from the user's query\n",
        "                and format it in YYYY-MM-DD format.\n",
        "                If no date is specified, answer with None value.\n",
        "                \"\"\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": question,\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    parsed_dates = adjust_time_filter_to_week(response.time_filter)\n",
        "\n",
        "    # parsed_dates를 순회하며 None인 경우도 처리\n",
        "    if parsed_dates:\n",
        "        start = parsed_dates['start_date']\n",
        "        end=parsed_dates['end_date']\n",
        "    else:\n",
        "        start=None\n",
        "        end = None\n",
        "\n",
        "    if start is None or end is None:\n",
        "        expr = None\n",
        "    else:\n",
        "        expr = f\"issue_date >= '{start.strftime('%Y%m%d')}' AND issue_date <= '{end.strftime('%Y%m%d')}'\"\n",
        "        expr = expr\n",
        "    return expr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "8U2KmujUcsV8"
      },
      "outputs": [],
      "source": [
        "def convert_to_list(example):\n",
        "    if isinstance(example[\"contexts\"], list):\n",
        "        contexts = example[\"contexts\"]\n",
        "    else:\n",
        "        try:\n",
        "            contexts = json.loads(example[\"contexts\"])\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON Decode Error: {example['contexts']} - {e}\")\n",
        "            contexts = []\n",
        "    return {\"contexts\": contexts}\n",
        "\n",
        "def generate_expr(question: str) -> dict:\n",
        "    expr = get_query_date(question)\n",
        "    return {\"expr\": expr}\n",
        "\n",
        "def reranking(docs, question, k=15):\n",
        "    chunks = [{\"id\": idx, \"issue_date\": doc.metadata['issue_date'],  \"text\": doc.page_content} for idx, doc in enumerate(docs)]\n",
        "    documents_with_metadata = [{\"text\": doc.page_content, \"metadata\": doc.metadata} for doc in docs]\n",
        "    reranked_results = rerank_results(query=question, chunks=chunks)\n",
        "\n",
        "    chunk_dict = {chunk[\"id\"]: chunk[\"text\"] for chunk in chunks}\n",
        "    top_k_results = [chunk_dict.get(label.chunk_id, \"\") for label in reranked_results.labels[:k] if label.chunk_id in chunk_dict]\n",
        "\n",
        "    reranked_results_with_metadata = []\n",
        "    for reranked_result in top_k_results:\n",
        "        page_content = reranked_result\n",
        "\n",
        "        matching_metadata = None\n",
        "        for doc in documents_with_metadata:\n",
        "            if doc[\"text\"] == page_content:\n",
        "                matching_metadata = doc[\"metadata\"]\n",
        "                break\n",
        "\n",
        "        document = Document(\n",
        "            metadata=matching_metadata,\n",
        "            page_content=page_content\n",
        "        )\n",
        "        reranked_results_with_metadata.append(document)\n",
        "\n",
        "    context_rerankedNbm25 = reranked_results_with_metadata\n",
        "    return context_rerankedNbm25\n",
        "\n",
        "text_prompt = PromptTemplate.from_template(\n",
        "'''\n",
        "today is '2025-01-25'. You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "If question has date expressions, context already filtered with the date expression, so ignore about the date and answer without it.\n",
        "Answer in Korean. Answer in detail.\n",
        "\n",
        "#Question:\n",
        "{question}\n",
        "#Context:\n",
        "{context}\n",
        "\n",
        "#Answer:'''\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "nRg0xKV7csV9"
      },
      "outputs": [],
      "source": [
        "question_answer_relevant = GroundednessChecker(\n",
        "  llm=ChatOpenAI(model='gpt-4o-mini', temperature=0), target='question-answer'\n",
        ").create()\n",
        "\n",
        "@chain\n",
        "def kill_table(result):\n",
        "    if question_answer_relevant.invoke({'question': result['question'], 'answer': result['text']}).score == 'no':\n",
        "        result['context'] = table_chain.invoke({'question': result['question']})\n",
        "    else:\n",
        "        result['context'] = result['text']\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "tmSVpgXOnsPq"
      },
      "outputs": [],
      "source": [
        "filepath = './chunked_jsonl/250313_text_semantic_per_80.jsonl'\n",
        "\n",
        "splitted_doc = []\n",
        "with open(filepath, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        if line.startswith('\\n('):\n",
        "            continue\n",
        "        data = json.loads(line)\n",
        "\n",
        "        doc = Document(\n",
        "            page_content=data['page_content'],\n",
        "            metadata=data['metadata']\n",
        "        )\n",
        "        splitted_doc.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# splitted_doc_text = []\n",
        "# for i in splitted_doc:\n",
        "#   if i.metadata['issue_date'][-2:] != '00':\n",
        "#     splitted_doc_text.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "enxxBnxqnsPq"
      },
      "outputs": [],
      "source": [
        "URI = 'http://127.0.0.1:19530'\n",
        "\n",
        "vectorstore_text = Milvus(\n",
        "    embedding_function=embeddings,\n",
        "    connection_args={'uri':URI},\n",
        "    index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "    collection_name='text_semantic_per_80_00_test'\n",
        ")\n",
        "\n",
        "# uuids = [str(uuid4()) for _ in range(len(splitted_doc_text))]\n",
        "\n",
        "# vectorstore_text.add_documents(\n",
        "#   documents=splitted_doc_text,\n",
        "#   ids=uuids\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorstore_predict = Milvus(\n",
        "    embedding_function=embeddings,\n",
        "    connection_args={'uri':URI},\n",
        "    index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "    collection_name='text_semantic_per_80_00'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "vt32Cqx40B2S"
      },
      "outputs": [],
      "source": [
        "\n",
        "milvus_retriever_text = vectorstore_text.as_retriever(\n",
        "    search_kwargs={'k':20}\n",
        ")\n",
        "\n",
        "bm25_retriever_text = KiwiBM25Retriever.from_documents(\n",
        "    splitted_doc_text\n",
        ")\n",
        "bm25_retriever_text.k = 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "8ZcPzPGo0B2T"
      },
      "outputs": [],
      "source": [
        "filepath = './chunked_jsonl/table_v7.jsonl'\n",
        "\n",
        "splitted_doc_table = []\n",
        "with open(filepath, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        if line.startswith('\\n('):\n",
        "            continue\n",
        "        data = json.loads(line)\n",
        "\n",
        "        doc = Document(\n",
        "            page_content=data['page_content'],\n",
        "            metadata=data['metadata']\n",
        "        )\n",
        "        splitted_doc_table.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "mIBqnNnj0B2U"
      },
      "outputs": [],
      "source": [
        "embeddings=UpstageEmbeddings(\n",
        "    model='solar-embedding-1-large-query'\n",
        ")\n",
        "\n",
        "vectorstore_table = Milvus(\n",
        "  embedding_function=embeddings,\n",
        "  connection_args={'uri':URI},\n",
        "  index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "  collection_name='table_v7'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "q8Ay3G9E0B2W"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "bm25_retriever_table = KiwiBM25Retriever.from_documents(\n",
        "    splitted_doc_table\n",
        ")\n",
        "bm25_retriever_table.k = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    # 각 문서의 issue_date와 page_content를 함께 출력하도록 포맷합니다.\n",
        "    return \"\\n\\n\".join(\n",
        "        f\"Issue Date: {doc.metadata.get('issue_date', 'Unknown')}\\nContent: {doc.page_content}\"\n",
        "        for doc in docs\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "CFn0Jq1zcsWB"
      },
      "outputs": [],
      "source": [
        "llm_text = ChatOpenAI(model='o1', temperature=1)\n",
        "\n",
        "answer = []\n",
        "\n",
        "text_chain = (\n",
        "    RunnableParallel(\n",
        "        question=itemgetter('question')\n",
        "    ).assign(expr = lambda x: get_query_date(x['question'])\n",
        "    ).assign(context_raw=lambda x: RunnableLambda(\n",
        "            lambda _: vectorstore_text.as_retriever(\n",
        "                search_kwargs={'expr': x['expr'], 'k': 25}\n",
        "            ).invoke(x['question'])\n",
        "        ).invoke({}),\n",
        "    ).assign(\n",
        "        context=lambda x: reranking(\n",
        "            list({doc.metadata.get(\"pk\"): doc for doc in (x['context_raw'])}.values()),\n",
        "            x['question'], 15\n",
        "        )\n",
        "    ).assign(\n",
        "        formatted_context=lambda x: format_docs(x['context'])\n",
        "    )\n",
        "    | RunnableLambda(\n",
        "        lambda x: {\n",
        "            \"question\": x['question'],\n",
        "            \"context\": x['formatted_context'],  \n",
        "        }\n",
        "    )\n",
        "    | text_prompt\n",
        "    | llm_text\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_text = ChatOpenAI(model='o1', temperature=1)\n",
        "\n",
        "answer = []\n",
        "\n",
        "predict_prompt = PromptTemplate.from_template(\n",
        "  '''You are future-predicting expert AI chatbot about financial.\n",
        "  주어진 정보는 retrieved context들이야. 이 정보를 바탕으로 미래를 예측해줘.\n",
        "\n",
        "  If one of the table or text says it doesn't know or it can't answer, don't mention with that.\n",
        "  주어진 예측을 한 근거도 함께 자세히 설명해줘. 왜 그런 예측을 어떤 걸 근거로 내놓았는지 알려줘.\n",
        "  Don't answer with the specific numbers.\n",
        "\n",
        "  #Question:\n",
        "  {question}\n",
        "\n",
        "  #Context:\n",
        "  {context}\n",
        "  '''\n",
        ")\n",
        "\n",
        "predict_expression = 'issue_date >= \"20241224\" AND issue_date <=\"20250124\"'\n",
        "predict_chain = (\n",
        "    RunnableParallel(\n",
        "        question=itemgetter('question')\n",
        "    ).assign(context=lambda x: RunnableLambda(\n",
        "            lambda _: vectorstore_text.as_retriever(\n",
        "                search_kwargs={'k': 20, 'expr':predict_expression}\n",
        "            ).invoke(x['question'])\n",
        "        ).invoke({}),\n",
        "    ).assign(\n",
        "        formatted_context=lambda x: format_docs(x['context'])\n",
        "    )\n",
        "    | RunnableLambda(\n",
        "        lambda x: {\n",
        "            \"question\": x['question'],\n",
        "            \"context\": x['formatted_context'],  \n",
        "        }\n",
        "    )\n",
        "    | predict_prompt\n",
        "    | llm_text\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_chain_2 = (\n",
        "    RunnableParallel(\n",
        "        question=itemgetter('question')\n",
        "    ).assign(expr = lambda x: get_query_date(x['question'])\n",
        "    ).assign(milvus=lambda x: RunnableLambda(\n",
        "            lambda _: vectorstore_text.as_retriever(\n",
        "                search_kwargs={'k': 25}\n",
        "            ).invoke(x['question'])\n",
        "        ).invoke({}),\n",
        "        bm25=lambda x: bm25_retriever_text.invoke(x['question'])\n",
        "    ).assign(\n",
        "        context=lambda x: reranking(\n",
        "            list({doc.metadata.get(\"pk\"): doc for doc in (x['milvus'] + x['bm25'])}.values()),\n",
        "            x['question'], 20\n",
        "        )\n",
        "    ).assign(\n",
        "        formatted_context=lambda x: format_docs(x['context'])\n",
        "    )\n",
        "    | RunnableLambda(\n",
        "        lambda x: {\n",
        "            \"question\": x['question'],\n",
        "            \"context\": x['formatted_context'],  \n",
        "        }\n",
        "    )\n",
        "    | text_prompt\n",
        "    | llm_text\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "4oqqtQXd0B2a"
      },
      "outputs": [],
      "source": [
        "table_prompt = PromptTemplate.from_template(\n",
        "'''You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved table to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Answer in Korean. Answer in detail.\n",
        "\n",
        "#Question:\n",
        "{question}\n",
        "#Context:\n",
        "{context}\n",
        "\n",
        "#Answer:'''\n",
        ")\n",
        "\n",
        "table_chain = (\n",
        "    RunnableParallel(\n",
        "        question=itemgetter('question')\n",
        "    ).assign(expr = lambda x: get_query_date(x['question'])\n",
        "    ).assign(milvus=lambda x: RunnableLambda(\n",
        "            lambda _: vectorstore_table.as_retriever(\n",
        "                search_kwargs={'expr': x['expr'], 'k': 10}\n",
        "            ).invoke(x['question'])\n",
        "        ).invoke({}),\n",
        "        bm25_raw=lambda x: bm25_retriever_table.invoke(x['question'])\n",
        "    ).assign(\n",
        "        bm25_filtered=lambda x: [\n",
        "            doc for doc in x[\"bm25_raw\"]\n",
        "            if not x[\"expr\"] or (\n",
        "                x[\"expr\"].split(\"'\")[1] <= doc.metadata.get(\"issue_date\", \"\") <= x[\"expr\"].split(\"'\")[3]\n",
        "            )\n",
        "        ],\n",
        "    ).assign(\n",
        "    context=lambda x: list({\n",
        "        doc.metadata.get(\"pk\"): doc \n",
        "        for doc in (x['milvus'] + x['bm25_filtered'])\n",
        "    }.values())\n",
        "    ).assign(\n",
        "        formatted_context=lambda x: format_docs(x['context'])\n",
        "    )\n",
        "    | RunnableLambda(\n",
        "        lambda x: {\n",
        "            \"question\": x['question'],\n",
        "            \"context\": x['formatted_context'],  #\n",
        "        }\n",
        "    )\n",
        "    | text_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "wu4CNzco0B2b"
      },
      "outputs": [],
      "source": [
        "llm_general = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "general_prompt = PromptTemplate.from_template(\n",
        "  '''You are question-answering AI chatbot about financial reports.\n",
        "  주어진 정보는 retrieved context들이야. 이 정보를 바탕으로 질문에 대해 자세히 설명해줘.\n",
        "\n",
        "  If one of the table or text says it doesn't know or it can't answer, don't mention with that.\n",
        "  And some questions may not be answered simply with context, but rather require inference. In those cases, answer by inference.\n",
        "\n",
        "  #Question:\n",
        "  {question}\n",
        "\n",
        "  #Context:\n",
        "  {context}\n",
        "  '''\n",
        ")\n",
        "\n",
        "date_chain = (\n",
        "    RunnableParallel(\n",
        "        question=itemgetter('question'),\n",
        "        text=text_chain,\n",
        "    )\n",
        "    | kill_table\n",
        "    | general_prompt\n",
        "    | llm_general\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "general_chain = (\n",
        "    RunnableParallel(\n",
        "        question=itemgetter('question'),\n",
        "        text=text_chain_2,\n",
        "    )\n",
        "    | kill_table\n",
        "    | general_prompt\n",
        "    | llm_general\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "KJ8ghEh5csWE"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Milvus\n",
        "\n",
        "embd = UpstageEmbeddings(\n",
        "    model='solar-embedding-1-large-query',\n",
        ")\n",
        "\n",
        "vectorstore_raptor = Milvus(\n",
        "    embedding_function=embd,\n",
        "    connection_args={'uri':URI},\n",
        "    index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "    collection_name='raptor_v3'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "OSzdlQ2b0B2e"
      },
      "outputs": [],
      "source": [
        "metadata_field_info = [\n",
        "  AttributeInfo(\n",
        "    name='source',\n",
        "    description='문서의 번호. 네 자리의 숫자와 \"호\"로 이루어져 있다. 현재 1090호부터 1120호까지 존재한다.',\n",
        "    type='string',\n",
        "  ),\n",
        "]\n",
        "\n",
        "prompt_query = get_query_constructor_prompt(\n",
        "  'summary of weekly financial report about bonds',\n",
        "  metadata_field_info\n",
        ")\n",
        "\n",
        "output_parser = StructuredQueryOutputParser.from_components()\n",
        "\n",
        "query_llm = ChatOpenAI(model='gpt-4-turbo-preview', temperature=0)\n",
        "query_constructor = prompt_query | query_llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "LNE7Gv1kcsWE"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.self_query.milvus import MilvusTranslator\n",
        "\n",
        "prompt_raptor = PromptTemplate.from_template(\n",
        "'''You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Answer in Korean. Answer in detail.\n",
        "If the context mentions an unrelated date, do not mention that part.\n",
        "Summarize and organize your answers based on the various issues that apply to the period.\n",
        "\n",
        "#Question:\n",
        "{question}\n",
        "#Context:\n",
        "{context}\n",
        "\n",
        "#Answer:'''\n",
        ")\n",
        "\n",
        "retriever_raptor = SelfQueryRetriever(\n",
        "  query_constructor=query_constructor,\n",
        "  vectorstore=vectorstore_raptor,\n",
        "  structured_query_translator=MilvusTranslator(),\n",
        "  search_kwargs={'k': 10}\n",
        ")\n",
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "raptor_chain = (\n",
        "    RunnableParallel(\n",
        "        question=itemgetter('question')\n",
        "    ).assign(expr = lambda x: get_query_date(x['question'])\n",
        "    ).assign(context=lambda x: retriever_raptor.invoke(x['question']))\n",
        "    | RunnableLambda(\n",
        "        lambda x: {\n",
        "            \"question\": x['question'],\n",
        "            \"context\": x['context'],\n",
        "        }\n",
        "    )\n",
        "    | prompt_raptor\n",
        "    | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "kevg3dtQcsWF"
      },
      "outputs": [],
      "source": [
        "raptor_date_chain = (\n",
        "    RunnableParallel(\n",
        "        question=itemgetter('question')\n",
        "    ).assign(expr = lambda x: get_query_date(x['question'])\n",
        "    ).assign(context=lambda x: RunnableLambda(\n",
        "            lambda _: vectorstore_raptor.as_retriever(\n",
        "                search_kwargs={'expr': x['expr'], 'k': 10}\n",
        "            ).invoke(x['question'])\n",
        "        ).invoke({})\n",
        "    )\n",
        "    | RunnableLambda(\n",
        "        lambda x: {\n",
        "            \"question\": x['question'],\n",
        "            \"context\": x['context'],\n",
        "        }\n",
        "    )\n",
        "    | prompt_raptor\n",
        "    | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "SvbguJfqcsWF"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_routing_2 = PromptTemplate.from_template(\n",
        "  '''You are an expert at routing a user question to the appropriate data source.\n",
        "\n",
        "If the user is asking for a brief summary, route it to the '요약' datasource.\n",
        "If the user is asking for more detailed or general information, route it to the '일반' datasource.\n",
        "If the user is asking for some prediction, route it to the '예측' datasource.\n",
        "\n",
        "Just answer with one word of datasource.\n",
        "\n",
        "Today is January 25th, 2025. Only classify as predictions asking about things after today.\n",
        "\n",
        "  <question>\n",
        "  {question}\n",
        "  </question>\n",
        "\n",
        "  datasource:'''\n",
        ")\n",
        "\n",
        "chain_routing_2 = (\n",
        "  {'question': RunnablePassthrough()}\n",
        "  | prompt_routing_2\n",
        "  | ChatOpenAI(model='gpt-4o-mini')\n",
        "  | StrOutputParser()\n",
        ")\n",
        "\n",
        "prompt_routing = PromptTemplate.from_template(\n",
        "  '''주어진 사용자 질문을 `날짜`, `호수`, `일반` 중 하나로 분류하세요. 한 단어 이상으로 응답하지 마세요.\n",
        "  If user question has the expression about date, route it to the '날짜' datasource.\n",
        "\n",
        "  <question>\n",
        "  {question}\n",
        "  </question>\n",
        "\n",
        "  Classification:'''\n",
        ")\n",
        "\n",
        "chain_routing = (\n",
        "  {'question': RunnablePassthrough()}\n",
        "  | prompt_routing\n",
        "  | ChatOpenAI(model='o1')\n",
        "  | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "K5SYwc4AcsWF"
      },
      "outputs": [],
      "source": [
        "def route_2(info):\n",
        "  if '요약' in info['topic'].lower():\n",
        "    print('raptor_date_chain')\n",
        "    return raptor_date_chain\n",
        "  elif '예측' in info['topic'].lower():\n",
        "    print('predict_chain')\n",
        "    return predict_chain\n",
        "  else:\n",
        "    print('date_chain')\n",
        "    return date_chain\n",
        "\n",
        "def route(info):\n",
        "  if '날짜' in info['topic'].lower():\n",
        "    info['topic'] = chain_routing_2.invoke(info['question'])\n",
        "    return route_2(info)\n",
        "  elif '호수' in info['topic'].lower():\n",
        "    print('raptor_chain')\n",
        "    return raptor_chain\n",
        "  else:\n",
        "    print('general_chain')\n",
        "    return general_chain\n",
        "\n",
        "\n",
        "full_chain = (\n",
        "  {'topic': chain_routing, 'question': itemgetter('question')}\n",
        "  | RunnableLambda(\n",
        "    route\n",
        "  )\n",
        "  | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "J2kYvPKecsWG"
      },
      "outputs": [],
      "source": [
        "filepath = './chunked_jsonl/image_v2.jsonl'\n",
        "\n",
        "splitted_doc_image = []\n",
        "with open(filepath, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        if line.startswith('\\n('):\n",
        "            continue\n",
        "        data = json.loads(line)\n",
        "\n",
        "        doc = Document(\n",
        "            page_content=data['page_content'],\n",
        "            metadata=data['metadata']\n",
        "        )\n",
        "        splitted_doc_image.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "VNuo52ExcsWG"
      },
      "outputs": [],
      "source": [
        "URI = 'http://127.0.0.1:19530'\n",
        "\n",
        "vectorstore_image = Milvus(\n",
        "    embedding_function=embeddings,\n",
        "    connection_args={'uri':URI},\n",
        "    index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "    collection_name='image_v4'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "xjBncZUkcsWG"
      },
      "outputs": [],
      "source": [
        "retriever_image = vectorstore_image.as_retriever(search_kwargs={'k': 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "-nXYKCIAcsWG"
      },
      "outputs": [],
      "source": [
        "query_retrieval_relevant = GroundednessChecker(\n",
        "  llm=ChatOpenAI(model='gpt-4o-mini', temperature=0), target='question-retrieval'\n",
        ").create()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "3ZSXtfghcsWG"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "\n",
        "def ask(question):\n",
        "    expr = get_query_date(question)\n",
        "    answer = full_chain.invoke({'question': question})\n",
        "    print(answer)\n",
        "    rc('font', family='Malgun Gothic')\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "    context = retriever_image.invoke(question, expr=expr)\n",
        "    for i in context:\n",
        "        rar = query_retrieval_relevant.invoke({'context': i, 'question': question})\n",
        "        if rar.score=='yes':\n",
        "            plt.title('참고 자료')\n",
        "            image_path = i.metadata['image'].replace('raw_pdf_copy3', 'parsed_pdf')\n",
        "            img = Image.open(image_path)\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
