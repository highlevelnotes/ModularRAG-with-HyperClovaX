{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 250603 LCEL\n",
        "\n",
        "* 250321_LCEL 파일의 chat과 embedding을 hyper clova 활용 모델로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6GUHybGwnsPh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import jsonlines\n",
        "from langchain.schema import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_naver.embeddings import ClovaXEmbeddings\n",
        "from langchain_milvus.vectorstores import Milvus\n",
        "from uuid import uuid4\n",
        "from langchain_naver.chat_models import ChatClovaX\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "import pandas as pd\n",
        "import pytz\n",
        "\n",
        "from datasets import Dataset\n",
        "from datetime import timedelta\n",
        "from operator import itemgetter\n",
        "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.chains.query_constructor.base import (\n",
        "  AttributeInfo,\n",
        "  StructuredQueryOutputParser,\n",
        "  get_query_constructor_prompt\n",
        ")\n",
        "from langchain_teddynote.evaluator import GroundednessChecker\n",
        "from langchain.retrievers.self_query.milvus import MilvusTranslator\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "import warnings\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "LP2hMGEp0B2P"
      },
      "outputs": [],
      "source": [
        "embeddings = ClovaXEmbeddings(\n",
        "    model='bge-m3'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel\n",
        "import instructor\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "class TimeFilter(BaseModel):\n",
        "    start_date: Optional[datetime] = None\n",
        "    end_date: Optional[datetime] = None\n",
        "\n",
        "class SearchQuery(BaseModel):\n",
        "    query: str\n",
        "    time_filter: TimeFilter\n",
        "\n",
        "class Label(BaseModel):\n",
        "    chunk_id: int = Field(description=\"The unique identifier of the text chunk\")\n",
        "    chain_of_thought: str = Field(\n",
        "        description=\"The reasoning process used to evaluate the relevance\"\n",
        "    )\n",
        "    relevancy: int = Field(\n",
        "        description=\"Relevancy score from 0 to 10, where 10 is most relevant\",\n",
        "        ge=0,\n",
        "        le=10,\n",
        "    )\n",
        "\n",
        "class RerankedResults(BaseModel):\n",
        "    labels: list[Label] = Field(description=\"List of labeled and ranked chunks\")\n",
        "\n",
        "    @field_validator(\"labels\")\n",
        "    @classmethod\n",
        "    def model_validate(cls, v: list[Label]) -> list[Label]:\n",
        "        return sorted(v, key=lambda x: x.relevancy, reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def adjust_time_filter_to_week(time_filter):\n",
        "    \"\"\"\n",
        "    특정 날짜(YYYY-MM-DD)가 주어진 경우, 해당 날짜를 포함하는 주(월~일)의\n",
        "    첫 번째 날(월요일)과 마지막 날(일요일)로 변환하는 함수.\n",
        "\n",
        "    :param time_filter: dict, {\"start_date\": datetime, \"end_date\": datetime}\n",
        "    :return: dict, {\"start_date\": datetime, \"end_date\": datetime}\n",
        "    \"\"\"\n",
        "    # Extract start_date and end_date from time_filter\n",
        "    start_date = time_filter.start_date\n",
        "    end_date = time_filter.end_date\n",
        "\n",
        "    # Handle the case where start_date or end_date is None\n",
        "    if start_date is None or end_date is None:\n",
        "        if start_date is not None and end_date is None:\n",
        "            start_of_week = start_date - timedelta(days=start_date.weekday())  # 월요일 찾기\n",
        "            end_of_week = start_of_week + timedelta(days=6)  # 해당 주 일요일 찾기\n",
        "\n",
        "            return {\n",
        "                \"start_date\": start_of_week.replace(hour=0, minute=0, second=0),\n",
        "                \"end_date\": end_of_week.replace(hour=23, minute=59, second=59)\n",
        "            }\n",
        "        elif end_date is not None and start_date is None:\n",
        "            start_of_week = end_date - timedelta(days=end_date.weekday())  # 월요일 찾기\n",
        "            end_of_week = start_of_week + timedelta(days=6)  # 해당 주 일요일 찾기\n",
        "\n",
        "            return {\n",
        "                \"start_date\": start_of_week.replace(hour=0, minute=0, second=0),\n",
        "                \"end_date\": end_of_week.replace(hour=23, minute=59, second=59)\n",
        "            }\n",
        "        else:\n",
        "            return None  # or return the time_filter as is if you prefer\n",
        "\n",
        "    # 날짜가 동일한 경우, 주의 첫 번째 날(월요일)과 마지막 날(일요일)로 변경\n",
        "    if start_date.year == end_date.year and start_date.month==end_date.month and start_date.day==end_date.day:\n",
        "        start_of_week = start_date - timedelta(days=start_date.weekday())  # 월요일 찾기\n",
        "        end_of_week = start_of_week + timedelta(days=6)  # 해당 주 일요일 찾기\n",
        "\n",
        "        return {\n",
        "            \"start_date\": start_of_week.replace(hour=0, minute=0, second=0),\n",
        "            \"end_date\": end_of_week.replace(hour=23, minute=59, second=59)\n",
        "        }\n",
        "\n",
        "    # 날짜가 다르면 기존 time_filter 유지\n",
        "    return {\n",
        "        \"start_date\": start_date,\n",
        "        \"end_date\": end_date\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_search_query_response(response: str, question: str) -> SearchQuery:\n",
        "    \"\"\"\n",
        "    ChatClovaX 응답을 SearchQuery 객체로 파싱\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 응답이 JSON 문자열이라고 가정\n",
        "        data = json.loads(response.content)\n",
        "        # time_filter가 null이면 빈 dict으로 변환\n",
        "        if data.get(\"time_filter\") is None:\n",
        "            data[\"time_filter\"] = {}\n",
        "        # query 필드가 없으면 원본 question을 사용\n",
        "        if \"query\" not in data:\n",
        "            data[\"query\"] = question\n",
        "        return SearchQuery(**data)\n",
        "    except Exception:\n",
        "        # 파싱 실패 시, 기본값 반환\n",
        "        return SearchQuery(query=question, time_filter=TimeFilter())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_query_date(question):\n",
        "    today = datetime(2025, 1, 25)\n",
        "    days_since_last_friday = (today.weekday() - 4) % 7\n",
        "    last_friday = today - timedelta(days=days_since_last_friday)\n",
        "    issue_date = last_friday.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # ChatClovaX 인스턴스 생성\n",
        "    chat = ChatClovaX(\n",
        "        model=\"HCX-005\",\n",
        "        temperature = 0\n",
        "    )\n",
        "\n",
        "    # 프롬프트: 반드시 SearchQuery 포맷(JSON)으로만 답변하게 유도\n",
        "    system_prompt = f\"\"\"\n",
        "    You are an AI assistant that extracts date ranges from financial queries.\n",
        "    The current report date is {issue_date}.\n",
        "    Your task is to extract the relevant date or date range from the user's query\n",
        "    and format it in YYYY-MM-DD format.\n",
        "    If no date is specified, answer with None value.\n",
        "    Return your answer as a JSON object in this format:\n",
        "    {{\n",
        "        \"query\": \"<원본 질문>\",\n",
        "        \"time_filter\": {{\"start_date\": \"YYYY-MM-DD\", \"end_date\": \"YYYY-MM-DD\"}} or {{\"start_date\": null, \"end_date\": null}}\n",
        "    }}\n",
        "    답변은 반드시 위 JSON 형태로만 해.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "    ]\n",
        "    \n",
        "    response = chat.invoke(messages)\n",
        "    # ChatClovaX 응답을 SearchQuery로 파싱\n",
        "    search_query = parse_search_query_response(response, question)\n",
        "\n",
        "    # adjust_time_filter_to_week는 기존 함수 그대로 사용\n",
        "    parsed_dates = adjust_time_filter_to_week(search_query.time_filter)\n",
        "\n",
        "    if parsed_dates:\n",
        "        start = parsed_dates['start_date']\n",
        "        end = parsed_dates['end_date']\n",
        "    else:\n",
        "        start = None\n",
        "        end = None\n",
        "\n",
        "    if start is None or end is None:\n",
        "        expr = None\n",
        "    else:\n",
        "        expr = f\"issue_date >= '{start.strftime('%Y%m%d')}' AND issue_date <= '{end.strftime('%Y%m%d')}'\"\n",
        "    return expr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_query_date(question):\n",
        "    today = datetime(2025, 1, 25)\n",
        "    days_since_last_friday = (today.weekday() - 4) % 7\n",
        "    last_friday = today - timedelta(days=days_since_last_friday)\n",
        "    issue_date = last_friday.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # ChatClovaX 인스턴스 생성\n",
        "    chat = ChatClovaX(\n",
        "        model=\"HCX-005\",\n",
        "        temperature = 0\n",
        "    )\n",
        "\n",
        "    # 프롬프트: 반드시 SearchQuery 포맷(JSON)으로만 답변하게 유도\n",
        "    system_prompt = f\"\"\"\n",
        "    You are an AI assistant that extracts date ranges from financial queries.\n",
        "    The current report date is {issue_date}.\n",
        "    Your task is to extract the relevant date or date range from the user's query\n",
        "    and format it in YYYY-MM-DD format.\n",
        "    If no date is specified, answer with None value.\n",
        "    Return your answer as a JSON object in this format:\n",
        "    {{\n",
        "        \"query\": \"<원본 질문>\",\n",
        "        \"time_filter\": {{\"start_date\": \"YYYY-MM-DD\", \"end_date\": \"YYYY-MM-DD\"}} or {{\"start_date\": null, \"end_date\": null}}\n",
        "    }}\n",
        "    답변은 반드시 위 JSON 형태로만 해.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "    ]\n",
        "    \n",
        "    response = chat.invoke(messages)\n",
        "    # ChatClovaX 응답을 SearchQuery로 파싱\n",
        "    search_query = parse_search_query_response(response, question)\n",
        "\n",
        "    # adjust_time_filter_to_week는 기존 함수 그대로 사용\n",
        "    parsed_dates = adjust_time_filter_to_week(search_query.time_filter)\n",
        "\n",
        "    if parsed_dates:\n",
        "        start = parsed_dates['start_date']\n",
        "        end = parsed_dates['end_date']\n",
        "    else:\n",
        "        start = None\n",
        "        end = None\n",
        "\n",
        "    if start is None or end is None:\n",
        "        expr = None\n",
        "    else:\n",
        "        expr = f\"issue_date >= '{start.strftime('%Y%m%d')}' AND issue_date <= '{end.strftime('%Y%m%d')}'\"\n",
        "    return expr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_list(example):\n",
        "    if isinstance(example[\"contexts\"], list):\n",
        "        contexts = example[\"contexts\"]\n",
        "    else:\n",
        "        try:\n",
        "            contexts = json.loads(example[\"contexts\"])\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON Decode Error: {example['contexts']} - {e}\")\n",
        "            contexts = []\n",
        "    return {\"contexts\": contexts}\n",
        "\n",
        "text_prompt = PromptTemplate.from_template(\n",
        "'''\n",
        "today is '2025-01-25'. You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "If question has date expressions, context already filtered with the date expression, so ignore about the date and answer without it.\n",
        "Answer in Korean. Answer in detail.\n",
        "\n",
        "#Question:\n",
        "{question}\n",
        "#Context:\n",
        "{context}\n",
        "\n",
        "#Answer:'''\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_answer_relevant = GroundednessChecker(\n",
        "  llm=ChatClovaX(model=\"HCX-005\"), target='question-answer'\n",
        ").create()\n",
        "\n",
        "@chain\n",
        "def kill_table(result):\n",
        "    if question_answer_relevant.invoke({'question': result['question'], 'answer': result['text']}).score == 'no':\n",
        "        result['context'] = table_chain.invoke({'question': result['question']})\n",
        "    else:\n",
        "        result['context'] = result['text']\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "URI = 'http://127.0.0.1:19530'\n",
        "\n",
        "text_db = Milvus(\n",
        "    embedding_function=embeddings,\n",
        "    connection_args = {'uri': URI},\n",
        "    index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "    collection_name='text_db'\n",
        ")\n",
        "\n",
        "image_db = Milvus(\n",
        "    embedding_function=embeddings,\n",
        "    connection_args = {'uri': URI},\n",
        "    index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "    collection_name='image_db'\n",
        ")\n",
        "\n",
        "raptor_db = Milvus(\n",
        "    embedding_function=embeddings,\n",
        "    connection_args = {'uri': URI},\n",
        "    index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "    collection_name='raptor_db'\n",
        ")\n",
        "\n",
        "table_db = Milvus(\n",
        "    embedding_function=embeddings,\n",
        "    connection_args = {'uri': URI},\n",
        "    index_params={'index_type': 'AUTOINDEX', 'metric_type': 'IP'},\n",
        "    collection_name='table_db'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "filepath = './chunked_jsonl/text_semantic_per_80.jsonl'\n",
        "\n",
        "splitted_doc_text = []\n",
        "with open(filepath, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        if line.startswith('\\n('):\n",
        "            continue\n",
        "        data = json.loads(line)\n",
        "\n",
        "        doc = Document(\n",
        "            page_content=data['page_content'],\n",
        "            metadata=data['metadata']\n",
        "        )\n",
        "        splitted_doc_text.append(doc)\n",
        "\n",
        "filepath = './chunked_jsonl/table_v7.jsonl'\n",
        "\n",
        "splitted_doc_table = []\n",
        "with open(filepath, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        if line.startswith('\\n('):\n",
        "            continue\n",
        "        data = json.loads(line)\n",
        "\n",
        "        doc = Document(\n",
        "            page_content=data['page_content'],\n",
        "            metadata=data['metadata']\n",
        "        )\n",
        "        splitted_doc_table.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_retriever_table = KiwiBM25Retriever.from_documents(\n",
        "    splitted_doc_table\n",
        ")\n",
        "\n",
        "bm25_retriever_table.k = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25_retriever_text = KiwiBM25Retriever.from_documents(\n",
        "    splitted_doc_text\n",
        ")\n",
        "bm25_retriever_text.k = 50\n",
        "\n",
        "bm25_2_retriever_text = KiwiBM25Retriever.from_documents(\n",
        "    splitted_doc_text\n",
        ")\n",
        "bm25_2_retriever_text.k = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    # 각 문서의 issue_date와 page_content를 함께 출력하도록 포맷합니다.\n",
        "    return \"\\n\\n\".join(\n",
        "        f\"Issue Date: {doc.metadata.get('issue_date', 'Unknown')}\\nContent: {doc.page_content}\"\n",
        "        for doc in docs\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "table_prompt = PromptTemplate.from_template(\n",
        "'''You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved table to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Answer in Korean. Answer in detail.\n",
        "\n",
        "#Question:\n",
        "{question}\n",
        "#Context:\n",
        "{context}\n",
        "\n",
        "#Answer:'''\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatClovaX(model='HCX-005', temperature=0)\n",
        "\n",
        "answer = []\n",
        "\n",
        "text_chain = (\n",
        "    RunnableParallel(\n",
        "        question=itemgetter('question')\n",
        "    ).assign(expr = lambda x: get_query_date(x['question'])\n",
        "    ).assign(context_raw=lambda x: RunnableLambda(\n",
        "            lambda _: text_db.as_retriever(\n",
        "                search_kwargs={'expr': x['expr'], 'k': 25}\n",
        "            ).invoke(x['question'])\n",
        "        ).invoke({}),\n",
        "    ).assign(\n",
        "        formatted_context=lambda x: format_docs(x['context_raw'])\n",
        "    )\n",
        "    | RunnableLambda(\n",
        "        lambda x: {\n",
        "            \"question\": x['question'],\n",
        "            \"context\": x['formatted_context'],  \n",
        "        }\n",
        "    )\n",
        "    | text_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "table_chain = (\n",
        "    RunnableParallel(\n",
        "        question=itemgetter('question')\n",
        "    ).assign(expr = lambda x: get_query_date(x['question'])\n",
        "    ).assign(milvus=lambda x: RunnableLambda(\n",
        "            lambda _: table_db.as_retriever(\n",
        "                search_kwargs={'expr': x['expr'], 'k': 10}\n",
        "            ).invoke(x['question'])\n",
        "        ).invoke({}),\n",
        "        bm25_raw=lambda x: bm25_retriever_table.invoke(x['question'])\n",
        "    ).assign(\n",
        "        bm25_filtered=lambda x: [\n",
        "            doc for doc in x[\"bm25_raw\"]\n",
        "            if not x[\"expr\"] or (\n",
        "                x[\"expr\"].split(\"'\")[1] <= doc.metadata.get(\"issue_date\", \"\") <= x[\"expr\"].split(\"'\")[3]\n",
        "            )\n",
        "        ],\n",
        "    ).assign(\n",
        "        context=lambda x: x['milvus'] + x['bm25_filtered']\n",
        "    )\n",
        "    | RunnableLambda(\n",
        "        lambda x: {\n",
        "            \"question\": x['question'],\n",
        "            \"context\": x['context'],\n",
        "        }\n",
        "    )\n",
        "    | table_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "general_prompt = PromptTemplate.from_template(\n",
        "  '''You are question-answering AI chatbot about financial reports.\n",
        "  주어진 두 개의 정보는 table과 text에서 가져온 정보들이야. 이 정보를 바탕으로 질문에 대해 자세히 설명해줘.\n",
        "  \n",
        "  If one of the table or text says it doesn't know or it can't answer, don't mention with that.\n",
        "  And some questions may not be answered simply with context, but rather require inference. In those cases, answer by inference. \n",
        "  \n",
        "  #Question:\n",
        "  {question}\n",
        "\n",
        "  #Text Answer:\n",
        "  {text}\n",
        "\n",
        "  #Table Answer:\n",
        "  {table}\n",
        "  '''\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "general_chain = (\n",
        "    RunnableParallel(\n",
        "        question=RunnablePassthrough(),\n",
        "        text=text_chain,\n",
        "        table=table_chain,\n",
        "    )\n",
        "    | general_prompt \n",
        "    | llm\n",
        ")\n",
        "\n",
        "\n",
        "predict_chain = (\n",
        "    RunnableParallel(\n",
        "        question=RunnablePassthrough(),\n",
        "        text=text_chain,\n",
        "        table=table_chain,\n",
        "    )\n",
        "    | general_prompt \n",
        "    | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata_field_info = [\n",
        "  AttributeInfo(\n",
        "    name='source',\n",
        "    description='문서의 번호. 네 자리의 숫자와 \"호\"로 이루어져 있다. 현재 1090호부터 1120호까지 존재한다.',\n",
        "    type='string',\n",
        "  ),\n",
        "]\n",
        "\n",
        "prompt_query = get_query_constructor_prompt(\n",
        "  'summary of weekly financial report about bonds',\n",
        "  metadata_field_info\n",
        ")\n",
        "\n",
        "output_parser = StructuredQueryOutputParser.from_components()\n",
        "query_constructor = prompt_query | llm | output_parser\n",
        "\n",
        "\n",
        "prompt_raptor = PromptTemplate.from_template(\n",
        "'''You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Answer in Korean. Answer in detail.\n",
        "If the context mentions an unrelated date, do not mention that part.\n",
        "Summarize and organize your answers based on the various issues that apply to the period.\n",
        "\n",
        "#Question:\n",
        "{question}\n",
        "#Context:\n",
        "{context}\n",
        "\n",
        "#Answer:'''\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever_raptor = SelfQueryRetriever(\n",
        "  query_constructor=query_constructor,\n",
        "  vectorstore=raptor_db,\n",
        "  structured_query_translator=MilvusTranslator(),\n",
        "  search_kwargs={'k': 10}\n",
        ")\n",
        "\n",
        "raptor_chain = (\n",
        "    RunnableParallel(\n",
        "        question=itemgetter('question')\n",
        "    ).assign(expr = lambda x: get_query_date(x['question'])\n",
        "    ).assign(context=lambda x: retriever_raptor.invoke(x['question']))\n",
        "    | RunnableLambda(\n",
        "        lambda x: {\n",
        "            \"question\": x['question'],\n",
        "            \"context\": x['context'],\n",
        "        }\n",
        "    )\n",
        "    | prompt_raptor\n",
        "    | llm\n",
        ")\n",
        "\n",
        "\n",
        "raptor_date_chain = (\n",
        "    RunnableParallel(\n",
        "        question=itemgetter('question')\n",
        "    ).assign(expr = lambda x: get_query_date(x['question'])\n",
        "    ).assign(context=lambda x: RunnableLambda(\n",
        "            lambda _: raptor_db.as_retriever(\n",
        "                search_kwargs={'expr': x['expr'], 'k': 10}\n",
        "            ).invoke(x['question'])\n",
        "        ).invoke({})\n",
        "    )\n",
        "    | RunnableLambda(\n",
        "        lambda x: {\n",
        "            \"question\": x['question'],\n",
        "            \"context\": x['context'],\n",
        "        }\n",
        "    )\n",
        "    | prompt_raptor\n",
        "    | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_routing = PromptTemplate.from_template(\n",
        "  '''주어진 사용자 질문을 `요약`, `예측`, 또는 `일반` 중 하나로 분류하세요. 한 단어 이상으로 응답하지 마세요.\n",
        "  \n",
        "  <question>\n",
        "  {question}\n",
        "  </question>\n",
        "  \n",
        "  Classification:'''\n",
        ")\n",
        "\n",
        "chain_routing = (\n",
        "  {'question': RunnablePassthrough()}\n",
        "  | prompt_routing\n",
        "  | llm\n",
        "  | StrOutputParser()\n",
        ")\n",
        "\n",
        "prompt_routing_2 = PromptTemplate.from_template(\n",
        "  '''주어진 사용자 질문을 `날짜`, `호수` 중 하나로 분류하세요. 한 단어 이상으로 응답하지 마세요.\n",
        "  \n",
        "  <question>\n",
        "  {question}\n",
        "  </question>\n",
        "  \n",
        "  Classification:'''\n",
        ")\n",
        "\n",
        "chain_routing_2 = (\n",
        "  {'question': RunnablePassthrough()}\n",
        "  | prompt_routing_2\n",
        "  | llm\n",
        "  | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'start_date'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-69-7975522c70c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraptor_date_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'question'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'2025년 1월 보고서 요약해줘'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'expr'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_query_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2025년 1월 보고서 요약해줘'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-46-660a4d7b6f35>\u001b[0m in \u001b[0;36mget_query_date\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtime_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mparsed_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madjust_time_filter_to_week\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_filter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparsed_dates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-43-fe9b20d9556b>\u001b[0m in \u001b[0;36madjust_time_filter_to_week\u001b[1;34m(time_filter)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \"\"\"\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Extract start_date and end_date from time_filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mstart_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_filter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mend_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_filter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'start_date'"
          ]
        }
      ],
      "source": [
        "raptor_date_chain({'question': '2025년 1월 보고서 요약해줘', 'expr': get_query_date('2025년 1월 보고서 요약해줘')})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_2(info):\n",
        "  if '날짜' in info['topic'].lower():\n",
        "    return raptor_date_chain\n",
        "  else:\n",
        "    return raptor_chain\n",
        "  \n",
        "def route(info):\n",
        "  if '요약' in info['topic'].lower():\n",
        "    return route_2(info)\n",
        "  elif '예측' in info['topic'].lower():\n",
        "    return predict_chain\n",
        "  else:\n",
        "    return general_chain\n",
        "\n",
        "\n",
        "full_chain = (\n",
        "  {'topic': chain_routing, 'question': itemgetter('question')}\n",
        "  | RunnableLambda(\n",
        "    route\n",
        "  )\n",
        "  | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever_image = image_db.as_retriever(search_kwargs={'k': 3})\n",
        "\n",
        "retrieval_answer_relevant = GroundednessChecker(\n",
        "  llm\n",
        ").create()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from PIL import Image\n",
        "\n",
        "def ask(question):\n",
        "    expr = get_query_date(question)\n",
        "    answer = full_chain.invoke({'question': question})\n",
        "    print(answer)\n",
        "    rc('font', family='Malgun Gothic')\n",
        "    plt.rcParams['axes.unicode_minus'] = False        \n",
        "    context = retriever_image.invoke(question, expr=expr)\n",
        "    for i in context:\n",
        "        rar = retrieval_answer_relevant.invoke({'context': i, 'answer': answer})\n",
        "        if rar.score=='yes':\n",
        "            plt.title('참고 자료')\n",
        "            image_path = i.metadata['image'].replace('raw_pdf_copy3', 'parsed_pdf')\n",
        "            img = Image.open(image_path)\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "안녕하세요! 사용자님의 질문에는 다음과 같은 내용이 포함되어 있습니다:\n",
            "\n",
            "**{'topic': '일반', 'question': '안녕?'}**\n",
            "\n",
            "제공된 텍스트와 테이블의 답변을 참고하여 답변드리겠습니다.\n",
            "\n",
            "사용자님이 인사말로 \"안녕?\"이라고 물어보셨는데, 이는 상대방에게 안부를 묻거나 인사를 건네는 표현입니다. 따라서 이에 대한 적절한 대답은 아래와 같습니다.\n",
            "\n",
            "\"안녕하세요! 반갑습니다. 저는 CLOVA X이며, 금융 관련 질문에 대해 도움을 드리기 위해 준비되어 있습니다. 궁금하신 부분이나 알고 싶은 정보가 있다면 언제든지 말씀해 주세요.\"\n",
            "\n",
            "위와 같이 인사와 함께 자신의 역할 및 도움을 드릴 수 있는 부분을 안내하는 것이 좋습니다. 추가적인 질문이나 도움이 필요하시면 부담 없이 문의해 주시기 바랍니다. 감사합니다.\n"
          ]
        },
        {
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"Modification of default value in 'parallel_tool_calls' is not allowed\", 'code': '40001'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-71-b7921ebb2769>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'안녕?'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-64-00acde894dc2>\u001b[0m in \u001b[0;36mask\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretriever_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mrar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretrieval_answer_relevant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'context'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'answer'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'yes'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'참고 자료'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py\u001b[0m in \u001b[0;36minvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3045\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3046\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3047\u001b[1;33m                         \u001b[0minput_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3048\u001b[0m         \u001b[1;31m# finish the root run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3049\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py\u001b[0m in \u001b[0;36minvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5428\u001b[0m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5429\u001b[0m     ) -> Output:\n\u001b[1;32m-> 5430\u001b[1;33m         return self.bound.invoke(\n\u001b[0m\u001b[0;32m   5431\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5432\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m         return cast(\n\u001b[0;32m    371\u001b[0m             \u001b[1;34m\"ChatGeneration\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m             self.generate_prompt(\n\u001b[0m\u001b[0;32m    373\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    955\u001b[0m     ) -> LLMResult:\n\u001b[0;32m    956\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m                 results.append(\n\u001b[1;32m--> 776\u001b[1;33m                     self._generate_with_cache(\n\u001b[0m\u001b[0;32m    777\u001b[0m                         \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1020\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"run_manager\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m             result = self._generate(\n\u001b[0m\u001b[0;32m   1023\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             )\n",
            "\u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\langchain_naver\\chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_request_payload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0m_convert_payload_messages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         response = self.client.create(\n\u001b[0m\u001b[0;32m    196\u001b[0m             \u001b[1;33m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0mextra_headers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"X-NCP-CLOVASTUDIO-REQUEST-ID\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf\"lcnv-{str(uuid.uuid4())}\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\openai\\_utils\\_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    912\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m    913\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         return self._post(\n\u001b[0m\u001b[0;32m    915\u001b[0m             \u001b[1;34m\"/chat/completions\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             body=maybe_transform(\n",
            "\u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m         )\n\u001b[1;32m-> 1242\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m     def patch(\n",
            "\u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 919\u001b[1;33m         return self._request(\n\u001b[0m\u001b[0;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Jo\\AppData\\Local\\Programs\\Python311\\Lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Re-raising status error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1023\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m         return self._process_response(\n",
            "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Modification of default value in 'parallel_tool_calls' is not allowed\", 'code': '40001'}}"
          ]
        }
      ],
      "source": [
        "ask('안녕?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
